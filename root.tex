%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper
\usepackage[colorlinks,linkcolor=black]{hyperref}

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed

\usepackage{amsfonts}
\usepackage{cite}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subcaption}% needed for subfigure
\usepackage{diagbox}
%\usepackage{biblatex}
%\usepackage{textcomp}
\usepackage{xargs}
\usepackage[pdftex,dvipsnames,table]{xcolor}
\usepackage{dirtree}
\usepackage{multirow}
\usepackage{verbatim}
\title{\LARGE \bf
SUSTech Points: An Efficient 3D Point Cloud Annotation Tool
}

\author{E Li$^{1}$,Shuaijun Wang$^{1,2}$,  Chengyang Li$^{1}$, Dachuan Li$^{1}$,Xiangbin Wu$^{3}$, and Qi Hao$^{1,*}$% <-this % stops a space
\thanks{This work is partially supported by the National Natural Science Foundation of China (No: 61773197), the Science and Technology Innovation Committee of Shenzhen City (No: GJHZ20170314114424152), the Nanshan District Science and Technology Innovation Bureau (No: LHTD20170007), and the Intel ICRI-IACV Research Fund (CG$\#$52514373).}
\thanks{$^{*}$Corresponding author: Qi Hao (hao.q@sustech.edu.cn).}
\thanks{$^{1}$Department of Computer Science and Engineering,
Southern University of Science and Technology, Shenzhen, Guangdong, China, 518055}
\thanks{$^{2}$Harbin Institute of Technology,
92 West Dazhi Street, Nan Gang District, Harbin, China, 150001}%
% <-this % stops a space
\thanks{$^{3}$ Intel Corporation}%
% <-this % stops a space
}


\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
The major challenges of labeling the 3D objects lie in reasonable structure annotation results, flexible tool, and high-precision annotation. This paper presents a novel annotation toolbox (i.e. SUSTech Points), which can successfully process the trade-off between time-consuming and dataset quality. The novelty of this work includes three-folds: (1) developing a flexible operation annotation tool for 3D point clouds; (2) developing a semi-autonomous method to annotate the same objects in the consecutive frames; (3) developing an intelligent plug-in being used to count the number and type of labeled objects. The method is tested with the online annotation under various scenes. And the experimental results have demonstrated the robust performance of the proposed method.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\section{INTRODUCTION}

Accurately annotated autonomous driving datasets are important for training and verification of various perception algorithms.
There are three main components for antonomous driving dataset annotation: (1) pre-process, (2) visulization, and (3) operation, as shown in Fig. 1.

In the preprocessing, various AI algorithms are used to ...

Visulization requires ...

Manual operation are still needed for correction, . 


Tools are required to develop 


Many efforts have been used to develop AI-based pre-propress, however
visualization and manual operation impose a great deal of technical challenges:
\begin{enumerate}
\item  Fast Error Checking. Data involve 2D and 3D, multiple perspectives, it is necessary to guarantee their consistency with fast speed.
\item  Easy Human-Computer interface, The great number of objects being both with irregular data and the large field of view, it is necessary to afford an easy human-computer interactive interface. 
\item  Flexible for extension. There are a lot correlations among frames, areas, modality, so the annotation platform should be scalable and extensible.
\end{enumerate}

2D/3D box-based units for editing 

Visulization methods are developed to improve VR

automation tools are developed for scalability and extension 

room for improvement: 

This paper presents 
\begin{enumerate}
	
	\item.
	\item.
	\item.
	
\end{enumerate}

The contributions of this work include
\begin{enumerate}

	\item developing an open-source 3D point annotation tool featuring  well-designed visualization and convenient operations.
	\item developing a series of functions such as auto-shrink, boundary-aware rotation and smart box initialization etc. to enable fast and accurate 3D box annotating.
	\item developing a novel registration-based inter-frame annotation transfer method.
\end{enumerate}

The rest of this paper is organized as follows. Section


\begin{figure}[tp]
	\centering
	% Requires \usepackage{graphicx}
	%\resizebox{\linewidth}{!}{
	\includegraphics[width=0.45\textwidth]{./figures/arch}\\ %\vspace{-0.3cm}
	\caption{An illustration of our annotation platform. Our annotation procedure includes 3 modules, which are pre-processing, visualization and operation, respectively. The pre-processing module is used to obtain the sensor calibration information, filter the noises in data, and initialize AI method-based annotation results. The visualization module is utilized to assist data annotation. And the operation module implements the 3D bounding box annotation in the point clouds with algorithm assistance and also checks the annotation error.}
	\label{fig:main-arch}
	\vspace{-0.3cm}
\end{figure}





\begin{comment}



%3D environment understanding plays an important role in the autonomous driving. There are many ways to build the 3D environment, which can be divided into 2 categories, including sensor-based and algorithm-based.
Recently, 3D environment understanding is gaining the attention in the research areas and industrial applications, like autonomous driving, etc. The 3D environment is often represented as point clouds. And the density of the point clouds reflects the quality of the 3D environment. A common method of obtaining 3D point clouds is by laser scanners, like LiDAR, etc. The labeling process of the point clouds have to deal with the following limitations/challenges:
\begin{figure}[htbp]
 \centering
 % Requires \usepackage{graphicx}
 %\resizebox{\linewidth}{!}{
 \includegraphics[width=0.45\textwidth]{./figures/arch}\\ %\vspace{-0.3cm}
 \caption{An illustration of our annotation platform. Our annotation procedure includes 3 modules, which are pre-processing, visualization and operation, respectively. The pre-processing module is used to obtain the sensor calibration information, filter the noises in data, and initialize AI method-based annotation results. The visualization module is utilized to assist data annotation. And the operation module implements the 3D bounding box annotation in the point clouds with algorithm assistance and also checks the annotation error.}
 \label{fig:arch}
 \vspace{-0.3cm}
\end{figure}

\begin{enumerate}
  \item Irregular density point cloud. In practical, the density of point clouds depends on distance between the sensor and measured objects, sensor type, and reflectivity of the measured objects, etc. Therefore, how to solve the different density of point clouds, including sparse and dense, becomes crucial.
  \item High-precision annotation. Compared with 2D image annotation, the annotation information of 3D point clouds becomes more complexity. It includes scale at each dimensionality axis, and heading angle of the object. How to obtain the accurate annotation information becomes important.
  \item Rich categories of object. The data-driven-based methods can distill the knowledge from the rich categories of the dataset, so that these methods can keep the favorable performance. The number of these objects is numerous. How to quickly annotate and balance these objects is also a critical issue.
  \item Reasonable data structure. The reasonable annotation data structure can efficiently decrease the store space and boost the speed of access.
  \item Flexible tool. Labeling the 3D objects is an enormous work, which includes annotation large amounts of objects and operation instructions for users. Therefore, developing a flexible labeling tool is much more important.
\end{enumerate}

The resolution of 3D point clouds is a formidable limitation for 3D annotation. Each frame point clouds obtained by LiDAR, like Velodyne, RoboSense, etc., has at least 10K 3D points. The measurement radius of  LiDAR is larger than 150 meters. So there are quite a few objects with a small number of points, extremely fewer than 10 points. These low-resolution 3D objects are even beyond the perception ability of humans. Another limitation is how to improve the annotation efficiency of the same object in consecutive frames, especially when the objects being farthest away from the sensor are with low-resolution 3D points.

An flexible annotation tool can improve the annotation efficiency and dataset quality. A considerable amount of research has been done on how to annotate 3D point clouds, like PointAtMe~\cite{pointatme}, LabelMe3D~\cite{LabelMe3D}, etc., during the last decade. The flexible annotation tool should be with easily applicable, intelligible and favorable visibility, as shown in the Fig.~\ref{fig:arch}. The intelligible plug-in of the annotation toolkit can promote the speed of the annotation process. The favorable visibility can raise the quality of the dataset, specifically in balancing the number of objects.

In this paper, we propose an efficient 3D point clouds annotation tool, with flexible operation, semi-autonomous annotation, and rapid deployment, to improve the annotation efficiency and assure the dataset quality. The contribution of this work as shown in the following.
\begin{itemize}
  \item Developing a flexible operation annotation tool for 3D point clouds;
  \item Developing a semi-autonomous method to annotate the same objects in the consecutive frames;
  \item Developing an intelligent plug-in being used to count the number and type of labeled objects.
\end{itemize}

The rest of this paper is organized as follows. Section~\ref{Realtedwork} introduces the related work on 3D objects annotation methods.
Section~\ref{setup-statement} describes the system setup and problem statement. Section~\ref{Approach} presents the proposed method.
Section~\ref{result} provides the experiment results and discussions. Section~\ref{conclusions} concludes this paper and outlines future work. To facilitate future research, the code has been released at "\url{http://www.baidu.com}".

\end{comment}



\begin{table*}[ht]
	\caption{A brief summary on 3D point clouds annotation methods}
	\resizebox{1.0\textwidth}{!}{
		\begin{tabular}{c||c|c|c|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|c||c|p{2cm}<{\centering}|p{2cm}<{\centering}|p{2cm}<{\centering}|p{2cm}<{\centering}}
		\hline
		\hline
		\multirow{2}{*}{} & \multicolumn{6}{c||}{Visualization}                         & \multicolumn{5}{c}{Operation}                 \\
		\cline{2-12}                        & Frame play  & Focus mode & Object color  & Box information & Camera switch & Auto-tracking & Auto-shrinking  & Boundary-aware rotation& Semi-auto annotation & Projective view operation& Automatic initial box generation \\  \hline
		PointAtMe~\cite{pointatme}         & $\times$    &   $\times$ &   $\times$    &   $\times$      &   $\times$    &  $\times$   &      +       &    +     &    +     &    +     &    +       \\ \hline
		LabelMe3D~\cite{LabelMe3D}         & $\times$    &   $\times$ &   $\times$    &   $\times$      &   $\times$    &  $\times$   &     ++       &   ++     &   ++     &   ++     &   ++       \\ \hline
		Riccardo~\cite{monica2017multi}    & $\times$    &   $\times$ &   $\times$    &   $\times$      &   $\times$    &  $\times$   &              &   ++     &   ++     &   ++     &   ++       \\ \hline
		Poly-RNN++~\cite{PolyRNN++}        & $\times$    &   $\times$ &   $\surd$     &   $\times$      &   $\times$    &  $\times$   &     +++      &  +++     &  +++     &  +++     &  +++       \\ \hline
		3D BAT~\cite{Zimmer20193DBA}       & $\times$    &   $\times$ &   $\times$    &                 &   $\times$    &  $\times$   &              &    +     &    +     &    +     &    +       \\ \hline
		LATTE~\cite{Wang2019LATTEAL}       & $\surd$     &   $\times$ &   $\times$    &   $\surd$       &   $\times$    &  $\surd$    &     ++       &   ++     &   ++     &   ++     &   ++       \\ \hline
		
		Supervise.ly~\cite{SUPERVISELY}      & $\surd$     &   $\times$ &   $\times$    &   $\surd$       &   $\times$    &  $\surd$    &     ++       &   ++     &   ++     &   ++     &   ++       \\ \hline
		
		Playment.io~\cite{Playment}       & $\surd$     &   $\times$ &   $\times$    &   $\surd$       &   $\times$    &  $\surd$    &     ++       &   ++     &   ++     &   ++     &   ++       \\ \hline
		
		scale.ai~\cite{scale}       & $\surd$     &   $\times$ &   $\times$    &   $\surd$       &   $\times$    &  $\surd$    &     ++       &   ++     &   ++     &   ++     &   ++       \\ \hline
		
		\textbf{SUSTech Points}            & $\surd$     &   $\surd$  &   $\surd$     &   $\surd$       &   $\surd$     &  $\surd$    &     +++      &  +++     &  +++     &  +++     &  +++       \\ \hline \hline
	\end{tabular}
	}
	\begin{tabular}{p{17.5cm}}
		The symbol ``$\surd$'' and ``$\times$'' mean the method being with/without the corresponding feature or function module, respectively. The number of ``$\textbf{+}$''s represent the performance of data processing, that is to say, the more the better. The ``Frame play'' means the annotation tool can play the data as a video. The ``Box information'' means that the bounding box can show the detail information of annotated object, such as turn left or right, \textit{etc.} The ``Camera switch''  means that the annotation tool can adaptively switch the observation angle of object, which is acquired by multiple cameras. The ``Auto-tracking'' means that the annotated object can be tracked within a continuous frame, which can improve the annotation speed. The ``Focus mode''  means that the object is automatically zoomed and centered at the projective view, when the object is being annotated. The ``Object color''  means the same class of objects being with the same color. This table lists the important factors which can improve the quality and speed of data annotation. For the sake of simplicity, this table does not list these common functions, like MOVE, RESIZE, and ROTATE, \textit{etc.} This table only concentrates on 3D objects annotation in point clouds, other than Poly-RNN++, which is an best 2D image annotation tool and as the function benchmark.
		% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
	\end{tabular}
	\label{tab:annotationMethods}
	\vspace{-0.5cm}
\end{table*}


\section{Related work}
\label{Realtedwork}
According to the types of annotation equipment, much of the research in the 3D annotation area in recent years can be classified as 2D-screen based and VR (virtual-reality) based methods. The table~\ref{tab:annotationMethods} gives the details comparison on current popular 3D point clouds annotation methods. In this table, the first 6 methods and ours are open source, and the remainning methods are commercial.
\subsection{2D-screen based annotation method}
2D-screen based methods can guarantee the 3D annotation quality and are without additional equipment to finish specific annotation tasks.
Details on these methods include the 2D-3D fusion-based and pure 3D-based annotation.
2D-3D fusion-based methods need the accurate spatial transformation relationship between 2D cameras and 3D sensors.
The spatial transformation relationship is calibrated by the stereo-vision algorithms, including Zhang's method~\cite{zhangzhegnyou}, Camera-LiDAR calibration~\cite{roadCalibration}, \emph{etc.}
But the performance of annotation strongly depends on the 2D-3D correspondence estimated from each sensor data. In addition, the 3D point are discrete and cannot find the accurate corresponding pixels at the image, which can increase the error of calibration and projection. A slice of these methods can only process the specific scenarios, like static scenarios, indoor scenarios, and so on.

Another kind of methods directly operates at the layer of 3D point clouds.
The method proposed by Yu \textit{et al.}~\cite{yu2012efficient} selects some points around the 2D projection of the object in 3D environment, which can approximately annotate the 3D object according to the structure-aware information. The cluster-based methods and segmentation-based method stand for the semi-autonomous and fully autonomous annotation method.
These methods outperform the VR-based annotation methods. However, these methods are with higher time consuming and computation complexity~\cite{pointatme}. It is worth noting that the method~\cite{monica2017multi} is the only one seriously evaluating the speed of their annotation tool. And we also adopt the metric~\cite{monica2017multi} to evaluate the speed of our label tool.
\subsection{VR-based annotation method}
VR equipment is helpful for the 3D environment understanding. However, the VR-based 3D annotation mathods are with the following limitations, including big annotation errors, highly time consuming. The VR can afford the immersion experience during wearing VR equipment. Taking advantages of VR, VR-based methods for 3D annotation are proposed in quite a few works~\cite{pointatme,wilkes20123dVR,coffey2011interactiveVR}. Since the VR-based methods depend on the hand gestures and 2 controllers to tune the bounding box of the 3D objects, it is difficult to accurately locate the boundaries of objects and implement fine-tuning the attitude, size, and angle in the short time. During the trade-off between efficiency and quality, the labeled bounding box by the VR-based methods is with bigger errors than that of the VR-free methods.

Another reason of the limitations is the point clouds belonging to the same object within an appropriate bounding box.
It needs multiple repeat operations to select the proper boundary points.
Specifically, the labeling process of the complex objects with irregular boundaries, like trees, zebra lines, etc., becomes more difficult than that of other objects with regular boundaries, such as cars, vans, and so on.

These 2 limitations increase the time consuming and decrease the annotation quality of 3D objects.
\subsection{The framework of annotation method}
The framework of annotation method includes explorer-based and software-based. The explorer-based framework is flexible to users for labeling 3D objects, which is based on the elements of the Web page. Therefore, the annotation framework, like LabelMe3D~\cite{LabelMe3D}, our proposed method, \emph{etc.}, can adaptively perform at any computer without additional operations. The software-based methods, like AppoloSuite~\cite{SUPERVISELY,wang2019apolloscape}, also need additional component that can ensure to execute. And it is difficult to synchronously and instantly update, which maybe cause the inconsistency of data, or other unpredictable results.

In this work, our proposed method directly annotate the objects at the 3D point clouds layer with the explorer-based framework. Our proposed method is with a convenience visualization module and intelligent operation module, as shown in table~\ref{tab:annotationMethods}, which can successfully process the trade-off between time-consuming and data-quality.

\section{3D annotation tool}
\label{3D annotation tool}
\begin{figure}[htp]
	\centering
	  \vspace{-0.1cm}
	% Requires \usepackage{graphicx}
	%\resizebox{\linewidth}{!}{
	\includegraphics[width=1.0\linewidth]{./figures/system-modules}\\ %\vspace{-0.3cm}
	 \caption{Details on our annotation platform. Our platform obtains the initialization results with the help of AI methods. The infrastructure module means the backend server, which is used to process data and provide network services.}
		\label{fig:main_ui}
		\vspace{-0.3cm}
   \end{figure}


\subsection{Visualization}

Visualization is important for both 3D box editing and reviewing. We describe visualization related features in this section.

\subsubsection{Sub-views}
\label{section:sub-views}
We split the main window into 1 main view and 5 sub-views, as shown in Fig.~\ref{fig:main_ui}. As in \cite{Zimmer20193DBA, more..}, the top/side/front projective views is provided to easily check the box alignment, different from \cite{Zimmer20193DBA} we hide environment points in these views to make it clean. we also provide an extra focused fusion view (bottom left sub-view in \ref{fig:main_ui}) to help check image details easily, otherwise a user will have to zoom in/out the context image.

\subsubsection{Focus-mode}
We provide a focus mode to facilitate detail check, as shown in Fig.~\ref{fig:focus-mode}. When activated through fast toolbox,the target object is automatically centered and zoomed in, most background hidden, while all editing features still applicable. Without this feature a user has to pan,  zoom and rotate the main view often, which we think is a lot operations and thus laborious.

\subsubsection{Camera auto-switch}
it's common to have multiple cameras and corresponding images to annotate, \cite{Zimmer20193DBA} displays all them on top of the main view, while \cite{} display one of them, we choose the latter way but with an automatic image switching feature. Specifically when an object is selected, the most relevant camera image is displayed, while manual selection is also possible.
note that this feature needs the calibration data to be available.

\subsubsection{Coloring}
All boxes and points inside are colored according to their types, and selected object are colored differently.

\subsubsection{Navigation}
\label{section:navigation}
\subsection{Basic 3D Box Editing}

\subsubsection{Edit in perspective view}
A box can be created with context menu(triggered by right click), the box dimension is initialized by object category. In main view(Fig.~\ref{fig:main_ui}), click a box to select it, click again (or click edit button in fast toolbox) to show/switch the operation handlers (Fig.~\ref{fig:box-mouse-edit}), drag the handlers to resize,rotate or move the box, click empty place (or press ESC key) to hide operation handlers or un-select the box.

\begin{figure}[!th]
	\centering	
	\includegraphics[width=0.9\linewidth]{./figures/focus-mode}\\
	\caption{Focus mode}
	\label{fig:focus-mode}
\end{figure}

When an object is selected (by mouse click, or navigate key shortcut), a floating \emph{fast-tool-box} shows next to it, as shown in Fig.~\ref{fig:main_ui}. A user can change object type, tracking id, activate focus mode, among others.


\begin{figure}[t]
	\centering
 \resizebox{\linewidth}{!}{
	\begin{subfigure}{0.3\linewidth}
		\includegraphics[height=2.5cm]{./figures/box-move}\\
		\caption{move}
	\end{subfigure}
	\begin{subfigure}{0.3\linewidth}
		\includegraphics[height=2.5cm]{./figures/box-rotate}\\
		\caption{rotate}
	\end{subfigure}	
	\begin{subfigure}{0.3\linewidth}
		\includegraphics[height=2.5cm]{./figures/box-resize}\\
		\caption{resize}
	\end{subfigure}
}
	\caption{Box editing in perspective view}
	\label{fig:box-mouse-edit}
\end{figure}

\subsubsection{Edit in projective sub-views}
Editing in perspective view is actually inconvenient, sometimes you have to change viewpoint for better operation. Actually the annotation is easily and should be checked in the side sub-views, thus we enable editing directly on these sub-views to make a better editing experience. when an object is selected and mouse enters one of the sub-views, the editing is activated automatically on this sub-view. user can drag dotted lines to rotate, resize or move the box, as can be seen in Fig.~\ref{fig:box-rotate-in-subview}.

All operations are summarized in Appendix \ref{app:operations}.


\subsubsection{Initial box generation}

A user can create a new box by right clicking on the object  and then select object type on popup context menu. Our tool provide the following features to help create box easily:

\begin{enumerate}
	\item the initial box direction is upright along the main view, 
	\item ...
\end{enumerate}


When a user create a new box by context menu to start annotating a new object, when user click object type name, a box is placed at the clicked position (center part of\ref{hptb}), the dimension is set according to the object type, and the heading direction(z-axis rotation) is set upward along the main view (better if the main view is rotated properly beforehand).
A euclidean distance based expansion algorithm is invoked to adjust the box to enclose all points of the same object. By this way the initial bounding box is almost ready.



\subsection{Assistant algorithms}
\begin{figure}[t]
	\centering

	\begin{subfigure}[t]{0.2\linewidth}
		\includegraphics[height=4cm]{./figures/points-enclosed}
		\caption{}

	\end{subfigure}\hfill
	\begin{subfigure}[t]{0.2\linewidth}
		\includegraphics[height=4cm]{./figures/adjust-naively}
		\caption{}
		\label{fig:box-rotate-in-subview}
	\end{subfigure}\hfill
	\begin{subfigure}[t]{0.2\linewidth}
		\includegraphics[height=4cm]{./figures/rotate-fail}
		\caption{}
	\end{subfigure}\hfill
	\begin{subfigure}[t]{0.2\linewidth}
		\includegraphics[height=4cm]{./figures/rotate-success}
		\caption{}
	\end{subfigure}\hfill
	\caption{For box in (a), which encloses all points of a car (easily done with auto-shrink operation), if we rotate it counter-clockwise to adjust the heading direction(b), part of the object will go outside(c), our tool can automatically adjust the dimension of the box as shown in (d)}
	\label{fig:boundary-aware-rotation}
\vspace{-0.3cm}
\end{figure}


\subsubsection{Auto-shrink}

Due to the unavoidable granularity of adjustment operations (of both keyboard and mouse), it's hard to determine precisely the border of a bounding box, our tool solves this problem by automatically shrink the borders to nearest inner points.

The user can drag the border or corner of a box to enclose all points of the object, after releasing the mouse key, the borders will automatically shrink to best positions.

\subsubsection{Boundary-aware rotation}

When annotate an object we normally perform the following 2 steps iteratively:
\begin{itemize}
	\item adjust position and/or scale
	\item adjust rotation	
\end{itemize}

When we rotate the box, some interior points may go outside(Fig.~\ref{fig:boundary-aware-rotation}), obviously it's better keep these points interior while adjust the dimension and/or position automatically, such that we don't have to adjust the dimension and/or position again. We emphasize that \emph{with this feature, to annotate an object is as easy as to enclose all points first and then rotate once}.




\subsubsection{Semi-auto annotation}
\label{semi-auto-anno}
In autonomous driving scenario, data sets are often organized by scenes\cite{Caesar2019nuScenesAM,Patil2019TheHD,lyft2019}, a scene is a sequence of frames shot at one continuous period of time. There are much similarities between neighboring frames, which makes it possible to transfer annotations among them. \cite{Zimmer20193DBA} copies annotations to next frame directly and let user do the adjustment.\cite{Wang2019LATTEAL} uses box estimation algorithm to automatically adjust the box, reusing previous annotated object size if appropriate. Our tool takes this way further by invoking registration algorithm \cite{Yang2016GoICPAG} to automatically adjust the position and rotation, reusing both the size and rotation previously annotated. In most cases our method produces  accurate enough annotation result.

\section{Implementation}
\label{Implementation}

\subsection{System Architecture}
We use web-based architecture, the raw data to be annotated is located in server. The end user can start working with a web browser and don't have to download the data set beforehand. The web server is implemented in Python and relies on CherryPy web framework\cite{cherrypy}. For front-end relies on \texttt{WebGL} library Three.js\cite{threejs}.


\subsection{Semi-auto annotation algorithm}

Semi-auto annotation of our tool is roughly a method to transfer annotation between frames, more accurately than naive paste or liner interpolation. The user need to first select (copy) a reference box, then paste it in appropriate position in target frame. A registration algorithm is invoked to compute the relative transform relation between the reference and target object, the target box is adjusted by the same transform relation. Note that the user need only to paste the box on the roughly right position.


\begin{figure}[ht]
	\centering
	\begin{subfigure}[t]{0.18\linewidth}
		\includegraphics[height=4cm]{./figures/reg-ref-3d}\\
		\caption{}\label{fig:box-ref}
	\end{subfigure}\hfill
	~
	\begin{subfigure}[t]{0.18\linewidth}
		\includegraphics[height=4cm]{./figures/reg-input-3d}\\
		\caption{}\label{fig:box-source}
	\end{subfigure}\hfill
	~
	\begin{subfigure}[t]{0.18\linewidth}
		\includegraphics[height=4cm]{./figures/reg-result-3d}\\
		\caption{}\label{fig:box-output}
	\end{subfigure}\hfill
	~
	\begin{subfigure}[t]{0.18\linewidth}
		\includegraphics[height=4cm]{./figures/reg-input}\\
		\caption{}\label{fig:reg-input}
	\end{subfigure}\hfill
	\begin{comment}
	~
	\begin{subfigure}[t]{0.18\linewidth}
		\includegraphics[height=3cm]{./figures/reg-tran}\\
		\caption{}\label{fig:reg-tran}
	\end{subfigure}\hfill
	~
	\begin{subfigure}[t]{0.18\linewidth}
		\includegraphics[height=3cm]{./figures/reg-result}\\
		\caption{}\label{fig:reg-output}
	\end{subfigure}\hfill
	\end{comment}
	
	\caption{Registration based bounding box transfer. \ref{fig:box-ref} is the reference object and its bounding box, \ref{fig:box-source} is the object in another frame with a initial  bounding box, \ref{fig:box-output} is the effect of box being adjusted.}
\end{figure}


Given source and target objects (denoted by $\mathcal{S,T}$ respectively)and their bounding boxes ($b_s,b_t$), a registration algorithm finds the transform matrix $T$ to match $\mathcal{S}$ with $\mathcal{T}$ , minimizing a distance function $dist$,
$$dist(T(\mathcal{S}),\mathcal{T})$$
where we represent points of $\mathcal{S,T}$ in their corresponding bounding box coordinates $B_s$ and $B_t$, the world coordinate system is denoted as $B_w$,

Assume point $s \in \mathcal{S}$ corresponds to $t \in \mathcal{T}$ after registration, then


\begin{align}
T s^{B_s} &= t^{B_t}\\
T B_s^{-1}s &= t^{B_t} \label{eq:eq1}
\end{align}

where $s^{B_s}$ means s represented in $B_s$ coordinate system, $s$ without upper index means default world coordinate system.

If we denote the new coordinate system of $\mathcal{S}$ as $B'_s$, then
\begin{equation} \label{eq:eq2}
s^{B'_s} = t^{B_t}
\end{equation}

by Eq.~\eqref{eq:eq1} and Eq.~\eqref{eq:eq2}, we have
\begin{align}
{B'_s} & = (T B_s^{-1})^{-1}\\
       & = B_s T^{-1} \label{eq:eq5}
\end{align}

If we use the reference object as target, object under annotation as source, Eq.~\eqref{eq:eq5} tells us we should apply the reverse transform of registration to the coordinates system, that is, to the box.


\subsection{Boundary-aware rotation}

A naive implementation is to recalculate the dimension of all original interior points and adjust the box accordingly, but it may fail when ground plane is present, after several rotation the box may become too big. One solution is to remove ground plane beforehand, but no algorithm can do this work perfectly as of our best knowledge, especially when the ground is not smooth (as can be seen often in KITTI data set\cite{Geiger2012CVPR} where many cars park on roadside). We use a simple yet effective method to tackle this problem. Specifically, we just ignore the most bottom part (0.2m by default) of the object when calculating the box dimension. Since a user is annotating the object, the pose of the object must be already roughly correct, so the bottom part can filter out ground points effectively. In the other hand, the object dimension estimation is not affected since almost all objects are much higher than 0.2m in traffic scenarios.


\section{Results and Evaluation}
\label {Metrics}

The evaluation consists of 3 parts, corresponding to 3 requirements we proposed in Section \ref{intro-requirements}.

\subsection{Detail Checking}
For this part, we use examples from KITTI 3D Object Detection data \cite{Geiger2012CVPR} set to demonstrate the effectiveness of checking annotation accuracy with our tool. We choose the same scene as show in  Fig. 5 of \cite{pointatme}.

Note although our tool supports all 9 degrees of freedom for 3D box, only 7 are labeled in KITTI data set, with pitch and tilt ignored, so in our evaluation we also ignore these 2 angles for fairness.

As show in Fig.~\ref{fig:annocheck}, we demonstrate that thanks to the 4 sub-views (Section~\ref{section:sub-views}) errors or inaccuracy spots are immediately and clearly identified by just navigating the objects one by one (Section~\ref{section:navigation}).

Tracking IDs can also be easily checked with the help of auto-follow  and stream-play features. For brevity reason we choose not to show examples here.

\begin{figure}
	\centering
	\begin{subfigure}{0.3\linewidth}
		\includegraphics[scale=0.2]{./figures/annocheck-0}
	\end{subfigure}
	~
	\begin{subfigure}{0.3\linewidth}
		\includegraphics[scale=0.2]{./figures/annocheck-1}
	\end{subfigure}
	~
	\begin{subfigure}{0.3\linewidth}
		\includegraphics[scale=0.2]{./figures/annocheck-2}
	\end{subfigure}
	
	\caption{Checking inaccuracies in KITTI 3D object detection data set}
	\label{fig:annocheck}
\end{figure}


\subsection{Object Annotating}

Annotation efficiency can be evaluated by annotation time and accuracy\cite{pointatme} \cite{Zimmer20193DBA}. The time shall be minimized and accuracy maximized. \cite{Zimmer20193DBA} counts interpolated annotations in so the time per object is extremely small, for fairness we use \cite{pointatme} as the baseline to compare our tool with.

As described in \cite{pointatme} and also indicated in previous section, the original KITTI labels is not suited for ground truth of our benchmarks. So we carefully annotated some scenes and use the result as ground truth, 3 users are employed to annotate the same set of data, with a few hours of training beforehand. We choose the same scene as used in \cite{pointatme}. For ground truth annotation the average time used per object is about 30 seconds (need edit).

For accuracy evaluation we follow the criteria  described in \cite{pointatme}, all points belonging to an object should be inside the bounding box and all points not belonging to it should be outside.

\begin{table}[h]
	\centering
	\caption{Results compared with baseline}
	\label{tab:metrics}
	\begin{tabular}{|l|c|c|c|c||c|c|c|c|}
		\hline
		\textbf{Method} & \textbf{Ours} & \textbf{PointAtME\cite{pointatme}} \\
		\hline
		\hline
		Time/Object (sec.) & xx & xx\\
		\hline
		Undo/Object & xx & xx\\
		\hline
		Errors (Points/Object) & xx & xx\\
		\hline
		FP ratio / Object & xx & xx\\
		\hline
		FN ratio / Object & xx & xx\\
		\hline
	\end{tabular}
\end{table}


\subsection{Annotation transfer}
For annotation transfer we mainly evaluate the accuracy. We use nuScenes data set \cite{Caesar2019nuScenesAM}, and transfer annotations between frames that have at least 0.5 second difference in time. As \cite{Caesar2019nuScenesAM} \cite{Patil2019TheHD} manually annotated the data set at a 2Hz frequency, 0.5s time gap is reasonable to demonstrate the usefulness of our tool.


\begin{table}[h]
	\centering
	\caption{Results compared with baseline}
	\label{tab:metrics}
	\begin{tabular}{|l|c|c|c|c||c|c|c|c|}
		\hline
		\textbf{Method} & \textbf{Ours} & \textbf{PointAtME\cite{pointatme}} \\
		\hline
		\hline
		Time/Object (sec.) & xx & xx\\
		\hline
		Undo/Object & xx & xx\\
		\hline
		Errors (Points/Object) & xx & xx\\
		\hline
		FP ratio / Object & xx & xx\\
		\hline
		FN ratio / Object & xx & xx\\
		\hline
	\end{tabular}
\end{table}


\section{CONCLUSIONS}
\label{conclusions}
A conclusion section is not required. Although a conclusion may review the main points of the paper, do not replicate the abstract as the conclusion. A conclusion might elaborate on the importance of the work or suggest applications and extensions.

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}


\section*{APPENDIX}

\subsection{Raw data orgnization}
In server, the raw data is organized by scenes, the directory structure is as in Fig.~\ref{fig:data-dir}.

\begin{figure}

	\dirtree{%
		.1 data.
		.2 scene1.
		.3 lidar.
		.4 frame0000.pcd.
		.4 ....
		.3 image.
		.4 left-camera.
		.5 frame0000.png.
		.5 ....
		.4 front.
		.5 frame0000.png.
		.5 ....
		.4 ....
		.3 calib.
		.4 left-camera.json.
		.4 front.json.
		.4 ....
		.3 label.
		.5 frame0000.json.
		.5 ....
		.2 scene2.
		.3 ....
		.2 ....
	}
	\caption{Raw data organization. The `label` folder is used to store annotation result. the `calib` folder stores lidar-camera calibration parameters, which is optional but requrired for 3d-2d fusion features. We list 3 cameras as an example, more cameras are also supported.}
	\label{fig:data-dir}
\end{figure}


\subsection{Operation summary}
\label{app:operations}
Table~\ref{table:keyboard_mainview} shows operations applicable in main view, Table~\ref{table:keyboard_subview} show operations applicable in side sub-views.

\begin{table}[h]
	\caption{Operations on main-view}
	\label{table:keyboard_mainview}
	\begin{center}
		\begin{tabular}{|c|l|}
			\hline
			\textbf{Key} & \textbf{Function}\\
			\hline
			1 & select previous object\\
			\hline
			2 & select next object\\
			\hline
			3 & previous frame\\
			\hline
			4 & next frame\\
			\hline
			r & rotate counterclockwise(yaw)\\
			\hline
			f & rotate clockwise(yaw)\\
			\hline
			t & reset box\\
			\hline
			g & reverse direction (yaw angle)\\
			\hline
			v & switch editing mode(Fig.~\ref{fig:box-mouse-edit})\\
			\hline
			Ctrl+s & save\\
			\hline
			Ctrl+c & copy reference box (\ref{semi-auto-anno})\\
			\hline
			Ctrl+z & undo\\
			\hline
			Ctrl+y & redo\\
			\hline
			Left drag & rotate view\\
			\hline
			Right drag & pan view\\
			\hline
			Wheel up & zoom in\\
			\hline
			Wheel down & zoom out\\
			\hline
			Ctrl+Left Drag (on main view)& mark points\\
			\hline
			Ctrl+Left click (on photo context)& draw polygon\\
			\hline
		\end{tabular}
	\end{center}
\end{table}


\begin{table}[h]
	\caption{Operations on sub-views}
	\label{table:keyboard_subview}
	\begin{center}
		\begin{tabular}{|c|l|c|l|}
			\hline
			\textbf{Key} & \textbf{Function}\\			
			\hline
			a & move left\\
			\hline
			s & move down\\
			\hline
			d & move right\\
			\hline
			w & move up\\
			\hline
			q & rotate left\\
			\hline
			e & rotate right\\
			\hline
			q & rotate left\\
			\hline
			e & rotate right\\
			\hline				
			Ctrl-q & rotate left with boundary aware\\
			\hline
			Ctrl-e & rotate right with boundary aware\\
			\hline
			Drag box border/corner & resizing box\\
			\hline
			Drag direction line & rotate box\\
			\hline
			Drag box border/corner with Ctrl hold & resizing box with auto-shrink\\
			\hline
			Drag direction line with Ctrl hold& rotate box with boundary aware\\
			\hline
			Drag box center & move box\\
			\hline
			Double click box border/corner & auto-shrinking\\
			\hline
			Double click box center & auto-shrinking all borders\\
			\hline
			Double click direction line& rotate upside-down (180 degree)\\
			\hline
		\end{tabular}
	
		 {\raggedright Note that all operation effects are relative to current sub-view.\par}
		 
	\end{center}
\end{table}
\section*{ACKNOWLEDGMENT}


\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliographystyle{IEEEtran}
\bibliography{MyReference}
\end{document}
